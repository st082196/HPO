{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "749ae57a-308e-4df1-88a5-6e03286c871a",
   "metadata": {
    "executionInfo": {
     "elapsed": 4367,
     "status": "ok",
     "timestamp": 1670775224957,
     "user": {
      "displayName": "Даниил Поляков",
      "userId": "15803614343846850762"
     },
     "user_tz": -180
    },
    "id": "749ae57a-308e-4df1-88a5-6e03286c871a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from functorch import vmap, grad, hessian\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ea46346-a9ec-4256-93ec-5a2cbec49007",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7764,
     "status": "ok",
     "timestamp": 1670775237772,
     "user": {
      "displayName": "Даниил Поляков",
      "userId": "15803614343846850762"
     },
     "user_tz": -180
    },
    "id": "9ea46346-a9ec-4256-93ec-5a2cbec49007",
    "outputId": "24f35c90-3ee7-416c-8186-b4849a31ba82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u = tensor([273.], device='cuda:0', grad_fn=<ProdBackward1>)\n",
      "d1 = tensor([273., 195., 147.], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "d2 = tensor([182.0000,  78.0000,  42.0000], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)\n",
      "laplacian = tensor([302.], device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Non-batched, non-vectorized\n",
    "x = torch.tensor([1., 2., 3.], requires_grad=True).to(device)\n",
    "u = torch.prod(x**2+x+1, -1, True)\n",
    "d1 = torch.autograd.grad(u, x, create_graph=True)[0]\n",
    "d2 = torch.stack([torch.autograd.grad(d1[i], x, create_graph=True)[0][i] for i in range(len(d1))])\n",
    "laplacian = torch.sum(d2, -1, True)\n",
    "\n",
    "print(f'u = {u}\\nd1 = {d1}\\nd2 = {d2}\\nlaplacian = {laplacian}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c5e49d-ea1f-45fc-96a5-c5031f3a7861",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1670775237773,
     "user": {
      "displayName": "Даниил Поляков",
      "userId": "15803614343846850762"
     },
     "user_tz": -180
    },
    "id": "57c5e49d-ea1f-45fc-96a5-c5031f3a7861",
    "outputId": "16357609-2a37-49ec-98f5-bb463a1cc11a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u = tensor([273.], device='cuda:0', grad_fn=<ProdBackward1>)\n",
      "d2 = tensor([182.0000,  78.0000,  42.0000], device='cuda:0',\n",
      "       grad_fn=<DiagBackward0>)\n",
      "laplacian = tensor([302.], device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Non-batched, vectorized\n",
    "x = torch.tensor([1., 2., 3.], requires_grad=True).to(device)\n",
    "u = torch.prod(x**2+x+1, -1, True)\n",
    "d1 = torch.autograd.grad(u, x, create_graph=True)[0]\n",
    "i = torch.eye(len(d1)).to(device)\n",
    "def get_vjp(i):\n",
    "    return torch.autograd.grad(d1, x, i, create_graph=True)\n",
    "d2 = torch.diag(vmap(get_vjp)(i)[0])\n",
    "laplacian = torch.sum(d2, -1, True)\n",
    "\n",
    "print(f'u = {u}\\nd2 = {d2}\\nlaplacian = {laplacian}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b3beee8-de5e-4892-8cd0-037112ce03fb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1670775242325,
     "user": {
      "displayName": "Даниил Поляков",
      "userId": "15803614343846850762"
     },
     "user_tz": -180
    },
    "id": "7b3beee8-de5e-4892-8cd0-037112ce03fb",
    "outputId": "098d4a8c-8ceb-4c6e-8a2e-1d6e449e8de3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u = tensor([[  273.],\n",
      "        [27993.]], device='cuda:0', grad_fn=<ProdBackward1>)\n",
      "d1 = tensor([[  273.,   195.,   147.],\n",
      "        [11997.,  9933.,  8463.]], device='cuda:0', grad_fn=<AddBackward0>)\n",
      "d2 = tensor([[ 182.0000,   78.0000,   42.0000],\n",
      "        [2666.0000, 1806.0000, 1302.0000]], device='cuda:0',\n",
      "       grad_fn=<StackBackward0>)\n",
      "laplacian = tensor([[ 302.],\n",
      "        [5774.]], device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Batched, non-vectorized\n",
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.]], requires_grad=True).to(device)\n",
    "u = torch.prod(x**2+x+1, -1, True)\n",
    "d1 = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "d2 = torch.stack([torch.autograd.grad(d1[:,i], x, grad_outputs=torch.ones_like(d1[:,i]), create_graph=True)[0][:,i] for i in range(d1.size()[1])], 1)\n",
    "laplacian = torch.sum(d2, -1, True)\n",
    "\n",
    "print(f'u = {u}\\nd1 = {d1}\\nd2 = {d2}\\nlaplacian = {laplacian}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "960748fa-fb93-4200-93fa-2b20ca7060fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1670775244973,
     "user": {
      "displayName": "Даниил Поляков",
      "userId": "15803614343846850762"
     },
     "user_tz": -180
    },
    "id": "960748fa-fb93-4200-93fa-2b20ca7060fd",
    "outputId": "b5a33ad0-6f23-4e35-a13b-f7a179bda358"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u = tensor([[  273.],\n",
      "        [27993.]], device='cuda:0')\n",
      "d2 = tensor([[[ 182.,   78.,   42.]],\n",
      "\n",
      "        [[2666., 1806., 1302.]]], device='cuda:0')\n",
      "laplacian = tensor([[ 302.],\n",
      "        [5774.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Batched, vectorized (hessian)\n",
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.]]).to(device)\n",
    "u = lambda x: torch.prod(x**2+x+1, -1, True)\n",
    "d2 = torch.diagonal(vmap(hessian(u))(x), dim1=-2, dim2=-1)\n",
    "laplacian = torch.sum(d2, -1)\n",
    "\n",
    "print(f'u = {u(x)}\\nd2 = {d2}\\nlaplacian = {laplacian}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a8b228d-dd92-4623-9f91-08861e789800",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1670775246604,
     "user": {
      "displayName": "Даниил Поляков",
      "userId": "15803614343846850762"
     },
     "user_tz": -180
    },
    "id": "7a8b228d-dd92-4623-9f91-08861e789800",
    "outputId": "7b17c4e2-d4ba-4fc2-bf6d-786b7c58ab66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u = tensor([[  273.],\n",
      "        [27993.]], device='cuda:0')\n",
      "d2 = tensor([[ 182.,   78.,   42.],\n",
      "        [2666., 1806., 1302.]], device='cuda:0')\n",
      "laplacian = tensor([[ 302.],\n",
      "        [5774.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Batched, vectorized (grad)\n",
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.]]).to(device)\n",
    "u = lambda x: torch.prod(x**2+x+1, -1, True)\n",
    "d2 = vmap(lambda x: torch.stack([grad(grad(lambda *x: u(torch.stack(x)).squeeze(), argnums=i), argnums=i)(*x) for i in range(len(x))]))(x)\n",
    "laplacian = torch.sum(d2, -1, True)\n",
    "\n",
    "print(f'u = {u(x)}\\nd2 = {d2}\\nlaplacian = {laplacian}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cfd99c59-8883-4248-927a-a4d7ba874d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual laplacian = tensor([[  1.0093],\n",
      "        [100.8718]], grad_fn=<SumBackward1>)\n",
      "Automatic laplacian = tensor([[  1.0093],\n",
      "        [100.8718]], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Manual laplacian\n",
    "class HelmholtzSolver(nn.Module):\n",
    "    def __init__(self, ndims, N, L, activation, bounds, g=lambda x: 0):\n",
    "        super(HelmholtzSolver, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(ndims, N), activation,\n",
    "            *[nn.Linear(N, N), activation]*(L-1),\n",
    "            nn.Linear(N, 1),\n",
    "        )\n",
    "        self.bounds = bounds\n",
    "        self.g = g\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # enforce boundary condition\n",
    "        return self.g(x) + torch.prod((x-self.bounds[0])*(self.bounds[1]-x), -1, True)*self.layers(x)\n",
    "\n",
    "\n",
    "class Sin(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)\n",
    "\n",
    "\n",
    "def laplacian(x):\n",
    "    z = [model.layers[0].weight @ x + model.layers[0].bias]\n",
    "    y = [torch.sin(z[0])]\n",
    "    dy1 = [torch.cos(z[0]).unsqueeze(1) * model.layers[0].weight]\n",
    "    dy2 = [-torch.sin(z[0]).unsqueeze(1) * model.layers[0].weight**2]\n",
    "    for i in range(1, L):\n",
    "        z.append(model.layers[2*i].weight @ y[i-1] + model.layers[2*i].bias)\n",
    "        y.append(torch.sin(z[i]))\n",
    "        dy1.append(torch.cos(z[i]).unsqueeze(1) * model.layers[2*i].weight @ dy1[i-1])\n",
    "        dy2.append(-torch.sin(z[i]).unsqueeze(1) * (model.layers[2*i].weight @ dy1[i-1])**2 + torch.cos(z[i]).unsqueeze(1) * model.layers[2*i].weight @ dy2[i-1])\n",
    "    y.append(model.layers[-1].weight @ y[-1] + model.layers[-1].bias)\n",
    "    dy1.append((model.layers[-1].weight @ dy1[-1]).squeeze(0))\n",
    "    dy2.append((model.layers[-1].weight @ dy2[-1]).squeeze(0))\n",
    "\n",
    "    a = model.bounds[0]\n",
    "    b = model.bounds[1]\n",
    "    dg2 = 0\n",
    "    du2 = dg2 + torch.prod(((x-a)*(b-x)).repeat(len(x), 1).fill_diagonal_(1), 1)*(-2*y[-1] + 2*(-2*x+a+b)*dy1[-1] + (-x**2+(a+b)*x-a*b)*dy2[-1])\n",
    "    return torch.sum(du2, -1, True)\n",
    "\n",
    "\n",
    "def laplacian_alt(x):\n",
    "    z = [model.layers[0].weight @ x + model.layers[0].bias]\n",
    "    y = [torch.sin(z[0])]\n",
    "    dy1 = [torch.cos(z[0]).unsqueeze(1) * model.layers[0].weight]\n",
    "    dy2 = [-torch.sin(z[0]).unsqueeze(1) * model.layers[0].weight**2]\n",
    "    for i in range(1, L):\n",
    "        z.append(model.layers[2*i].weight @ y[i-1] + model.layers[2*i].bias)\n",
    "        y.append(torch.sin(z[i]))\n",
    "        dy1.append(torch.cos(z[i]).unsqueeze(1) * model.layers[2*i].weight @ dy1[i-1])\n",
    "        dy2.append(-torch.sin(z[i]).unsqueeze(1) * (model.layers[2*i].weight @ dy1[i-1])**2 + torch.cos(z[i]).unsqueeze(1) * model.layers[2*i].weight @ dy2[i-1])\n",
    "    y.append(model.layers[-1].weight @ y[-1] + model.layers[-1].bias)\n",
    "    dy1.append((model.layers[-1].weight @ dy1[-1]).squeeze(0))\n",
    "    dy2.append((model.layers[-1].weight @ dy2[-1]).squeeze(0))\n",
    "\n",
    "    a = model.bounds[0]\n",
    "    b = model.bounds[1]\n",
    "    prod_matrix = ((x-a)*(b-x)).repeat(len(x), 1)\n",
    "    for i in range(len(x)):\n",
    "        prod_matrix[i,i] = 1\n",
    "    dg2 = 0\n",
    "    du2 = dg2 + torch.prod(prod_matrix, 1)*(-2*y[-1] + 2*(-2*x+a+b)*dy1[-1] + (-x**2+(a+b)*x-a*b)*dy2[-1])\n",
    "    return torch.sum(du2, -1, True)\n",
    "\n",
    "\n",
    "def laplacian_multiactivation(x):\n",
    "    if isinstance(activation, Sin):\n",
    "        a = torch.sin\n",
    "        da1 = lambda x: torch.cos(x)\n",
    "        da2 = lambda x: -torch.sin(x)\n",
    "    if isinstance(activation, nn.Tanh):\n",
    "        a = torch.tanh\n",
    "        da1 = lambda x: 1 - torch.tanh(x)**2\n",
    "        da2 = lambda x: 2*torch.tanh(x)*(torch.tanh(x)**2 - 1)\n",
    "    if isinstance(activation, nn.Sigmoid):\n",
    "        a = torch.sigmoid\n",
    "        da1 = lambda x: 1/(torch.exp(-x) + 2 + torch.exp(x))\n",
    "        da2 = lambda x: (torch.exp(-x) - torch.exp(x))/(torch.exp(-x) + 2 + torch.exp(x))**2\n",
    "\n",
    "    z = [model.layers[0].weight @ x + model.layers[0].bias]\n",
    "    y = [a(z[0])]\n",
    "    dy1 = [da1(z[0]).unsqueeze(1) * model.layers[0].weight]\n",
    "    dy2 = [da2(z[0]).unsqueeze(1) * model.layers[0].weight**2]\n",
    "    for i in range(1, L):\n",
    "        z.append(model.layers[2*i].weight @ y[i-1] + model.layers[2*i].bias)\n",
    "        y.append(a(z[i]))\n",
    "        dy1.append(da1(z[i]).unsqueeze(1) * model.layers[2*i].weight @ dy1[i-1])\n",
    "        dy2.append(da2(z[i]).unsqueeze(1) * (model.layers[2*i].weight @ dy1[i-1])**2 + da1(z[i]).unsqueeze(1) * model.layers[2*i].weight @ dy2[i-1])\n",
    "    y.append(model.layers[-1].weight @ y[-1] + model.layers[-1].bias)\n",
    "    dy1.append((model.layers[-1].weight @ dy1[-1]).squeeze(0))\n",
    "    dy2.append((model.layers[-1].weight @ dy2[-1]).squeeze(0))\n",
    "\n",
    "    b1 = model.bounds[0]\n",
    "    b2 = model.bounds[1]\n",
    "    prod_matrix = ((x-b1)*(b2-x)).repeat(len(x), 1)\n",
    "    for i in range(len(x)):\n",
    "        prod_matrix[i,i] = 1\n",
    "    dg2 = 0\n",
    "    du2 = dg2 + torch.prod(prod_matrix, 1)*(-2*y[-1] + 2*(-2*x+b1+b2)*dy1[-1] + (-x**2+(b1+b2)*x-b1*b2)*dy2[-1])\n",
    "    return torch.sum(du2, -1, True)\n",
    "\n",
    "\n",
    "ndims = 3\n",
    "N = 10\n",
    "L = 5\n",
    "activation = nn.Sin()\n",
    "bounds = [0, 1]\n",
    "model = HelmholtzSolver(ndims, N, L, activation, bounds).to(device)\n",
    "# x = torch.tensor([1., 2., 3.]).requires_grad_()\n",
    "x = torch.tensor([[1., 2., 3.], [4., 5., 6.]]).requires_grad_()\n",
    "\n",
    "print(f'Manual laplacian = {vmap(laplacian_multiactivation)(x)}')\n",
    "\n",
    "u = model(x)\n",
    "d1 = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "d2 = torch.stack([torch.autograd.grad(d1[:,i], x, grad_outputs=torch.ones_like(d1[:,i]), create_graph=True)[0][:,i] for i in range(d1.size()[1])], 1)\n",
    "laplacian = torch.sum(d2, -1, True)\n",
    "print(f'Automatic laplacian = {laplacian}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
