#%% Read data
with open(f'{directory}/output.csv') as output:
    data = list(csv.reader(output))
    del data[0]
    epoch_data, time_data, RMSE_data = zip(*data)


#%% Train with max_stall_epochs

# Parameters
directory = 'num_data=1e4, batch_size=1e3, Adam()'
num_terms = 2
bounds = [-pi, pi]
num_data = 10000
batch_size = 1000
max_time = 5
max_stall_epochs = float('inf')
tolerance = 1e-5
device = 'cpu'

# Train
model = SinApproximator(num_terms).to(device)
optimizer = torch.optim.Adam(model.parameters())
epoch = elapsed_time = best_epoch = 0
best_RMSE = float('inf')
time_data = []
RMSE_data = []
start_time = timestamp = time()
print(f'epoch | time | RMSE')
while elapsed_time < max_time and epoch - best_epoch < max_stall_epochs:
    epoch += 1
    train(model, x_train, y_train, optimizer, batch_size)
    pred, RMSE = test(model, x_test, y_test)
    elapsed_time = time() - start_time
    if RMSE/best_RMSE < 1 - tolerance:
        best_epoch = epoch
        best_RMSE = RMSE
    time_data.append(elapsed_time)
    RMSE_data.append(RMSE.cpu())
    if time() - timestamp >= 1:
        print(f'{epoch:5} | {elapsed_time:4.0f} | {RMSE:.4f}')
        timestamp = time()


#%% autograd.grad demonstration
x = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)
u = torch.prod(x**2+x+1, 1, True)
d1 = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0] # d1 = [d_u/d_x[0], d_u/d_x[1]]
d2 = torch.stack([torch.autograd.grad(d1[:,i], x, grad_outputs=torch.ones_like(d1[:,i]), create_graph=True)[0][:,i] for i in range(d1.size()[1])], 1) # d2 = [d_d1[0]/d_x[0], d_d1[1]/d_x[1]]
# d2 = torch.autograd.grad(d1, x, grad_outputs=torch.ones_like(d1), create_graph=True)[0] # d2 = [d_d1[0]/d_x[0] + d_d1[0]/d_x[1], d_d1[1]/d_x[0] + d_d1[1]/d_x[1]]
print(f'x = {x}\nu = {u}\nd1 = {d1}\nd2 = {d2}')

#%%
clc
x = [1, 2]
u = (x(1)^2+x(1)+1)*(x(2)^2+x(2)+1)
d1 = [(2*x(1)+1)*(x(2)^2+x(2)+1), (x(1)^2+x(1)+1)*(2*x(2)+1)]
d2 = [2*(x(2)^2+x(2)+1), 2*(x(1)^2+x(1)+1)]
d2_alt = [2*(x(2)^2+x(2)+1) + (2*x(1)+1)*(2*x(2)+1), (2*x(1)+1)*(2*x(2)+1) + (x(1)^2+x(1)+1)*2]

#%% Iterate over hyperparameters
ndims = 5
N = [256] # number of nodes per hidden layer
L = [3] # number of hidden layers
activation = [nn.Sigmoid(), nn.Tanh(), Sin()] # activation function
bounds = [0, 1]
num_data = [100000]
use_sobol = [False] # whether to generate training points using Sobol sequence (True) or uniformly (False)
batch_size = [1024]
lr = [1e-3]

for N, L, activation, num_data, use_sobol, batch_size, lr in itertools.product(N, L, activation, num_data, use_sobol, batch_size, lr):
    name = f'{ndims},{N},{L},{activation},{bounds},{num_data},{use_sobol},{batch_size},{lr}'


new_x = rng.normal(x, (bounds[:,1]-bounds[:,0])/2)
        for dim in range(len(x)):
            if new_x[dim] < bounds[dim,0]:
                new_x[dim] = rng.uniform(bounds[dim,0], x[dim])
            if new_x[dim] > bounds[dim,1]:
                new_x[dim] = rng.uniform(x[dim], bounds[dim,1])

#%% Scrambled Sobol sequence performance test
import torch
from time import time
device = 'gpu'
ndims = 5
n = 100000000

start_time = time()
sobolengine = torch.quasirandom.SobolEngine(ndims, True, 2022)
x = sobolengine.draw(n)
print(time()-start_time) # 2.9415950775146484

start_time = time()
sobolengine = torch.quasirandom.SobolEngine(ndims, False, 2022)
x = sobolengine.draw(n)
print(time()-start_time) # 3.215217351913452
# No performance difference

#%% Restore Tune run
tuner = tune.Tuner.restore(os.path.join('ray_results', name), tune.with_resources(trainable, resources={'cpu': 1, 'gpu': 1}))

#%% Old train and validation loops
def train_old(model, optimizer, x, batch_size=1):
    model.train()
    SSE_f = 0
    for i in range(0, len(x), batch_size):
        x_batch = x[i:i+batch_size]
        pred = model(x_batch)
        loss = SSE_f_fn(model, x_batch, pred)
        SSE_f += loss.item()
        loss /= len(x_batch)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    MSE_f = SSE_f/len(x)
    return MSE_f


def test_old(model, x, u, batch_size=1):
    model.eval()
    SSE_f = 0
    pred_list = []
    for i in range(0, len(x), batch_size):
        x_batch = x[i:i+batch_size]
        pred = model(x_batch)
        SSE_f += SSE_f_fn(model, x_batch, pred).item()
        pred_list.append(pred.detach())
    pred = torch.cat(pred_list)
    MSE_f = SSE_f/len(x)
    RMSE_u = torch.sqrt(torch.mean((pred-u)**2)).item()
    return MSE_f, RMSE_u