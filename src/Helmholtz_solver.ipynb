{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fc2d0ba-7d1d-4807-ac11-f7d40e2a0f47",
   "metadata": {
    "id": "5fc2d0ba-7d1d-4807-ac11-f7d40e2a0f47",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.func'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9560\\4243000109.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhessian\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLBFGS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExponentialLR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch.func'"
     ]
    }
   ],
   "source": [
    "#@title Initialization\n",
    "import os\n",
    "import itertools\n",
    "import csv\n",
    "import torch\n",
    "from math import pi, sqrt\n",
    "from time import time\n",
    "from torch import nn\n",
    "from torch.func import vmap, hessian\n",
    "from torch.optim import Adam, LBFGS\n",
    "from torch.optim.lr_scheduler import ExponentialLR, ReduceLROnPlateau\n",
    "from ray import tune\n",
    "from ray.air import session, RunConfig\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.search.bohb import TuneBOHB\n",
    "from ray.tune.schedulers import ASHAScheduler, HyperBandForBOHB\n",
    "from matplotlib.pyplot import figure, subplots, plot, xlabel, ylabel, title, savefig, close\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "class HelmholtzSolver(nn.Module):\n",
    "    def __init__(self, ndims, N, L, activation, bounds, g=lambda x: 0):\n",
    "        super(HelmholtzSolver, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(ndims, N), activation,\n",
    "            *[nn.Linear(N, N), activation]*(L-1),\n",
    "            nn.Linear(N, 1),\n",
    "        )\n",
    "        self.bounds = bounds\n",
    "        self.g = g\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # enforce boundary condition\n",
    "        return self.g(x) + torch.prod((x-self.bounds[0])*(self.bounds[1]-x), -1, True)*self.layers(x)\n",
    "\n",
    "\n",
    "class Sin(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)\n",
    "\n",
    "\n",
    "class Atan(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.atan(x)/(pi/2)\n",
    "\n",
    "\n",
    "def SSE_f_fn(model, x, u):\n",
    "    d1 = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    d2 = torch.diagonal(vmap(lambda I: torch.autograd.grad(d1, x, grad_outputs=torch.ones_like(d1)*I, create_graph=True)[0], 1)(torch.eye(ndims, device=device)), 0, 0, 2)\n",
    "    return torch.sum((torch.sum(d2, -1, True) + u + (ndims*4*pi**2 - 1)*torch.prod(torch.sin(2*pi*x), -1, True))**2)\n",
    "\n",
    "\n",
    "def train(model, optimizer, num_points, use_sobol=True, batch_size=1024, max_time=None, MSE_f=0, i=0):\n",
    "    start_time = time()\n",
    "    if max_time is None:\n",
    "        max_time = float('inf')\n",
    "    model.train()\n",
    "    torch.manual_seed(2022)\n",
    "    torch.rand(i, ndims, device=device)\n",
    "    if use_sobol:\n",
    "        sobolengine = torch.quasirandom.SobolEngine(ndims, True, 2022)\n",
    "        sobolengine.fast_forward(i)\n",
    "    SSE_f = MSE_f*i\n",
    "    while i < num_points and time() - start_time < max_time:\n",
    "        n = min(batch_size, num_points - i)\n",
    "        if use_sobol:\n",
    "            x = (sobolengine.draw(n)*(bounds[1] - bounds[0]) + bounds[0]).to(device).requires_grad_()\n",
    "        else:\n",
    "            x = (torch.rand(n, ndims, device=device)*(bounds[1] - bounds[0]) + bounds[0]).requires_grad_()\n",
    "        pred = model(x)\n",
    "        loss = SSE_f_fn(model, x, pred)\n",
    "        SSE_f += loss.item()\n",
    "        loss /= n\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        i += n\n",
    "    if i > 0:\n",
    "        MSE_f = SSE_f/i\n",
    "    return MSE_f, i\n",
    "\n",
    "\n",
    "def test(model, num_points, batch_size=1):\n",
    "    model.eval()\n",
    "    SSE_f, SSE_u = 0, 0\n",
    "    torch.manual_seed(2023)\n",
    "    for i in range(0, num_points, batch_size):\n",
    "        x = (torch.rand(min(batch_size, num_points - i), ndims, device=device)*(bounds[1] - bounds[0]) + bounds[0]).requires_grad_()\n",
    "        u = torch.prod(torch.sin(2*pi*x), -1, True)\n",
    "        pred = model(x)\n",
    "        SSE_f += SSE_f_fn(model, x, pred).item()\n",
    "        SSE_u += torch.sum((pred-u)**2).item()\n",
    "    MSE_f = SSE_f/num_points\n",
    "    RMSE_u = sqrt(SSE_u/num_points)\n",
    "    return MSE_f, RMSE_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57cd9f3-0ad1-4529-9a4a-80dce2979e21",
   "metadata": {
    "id": "d57cd9f3-0ad1-4529-9a4a-80dce2979e21",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Single model\n",
    "\n",
    "# Parameters\n",
    "directory = 'models/ndims=2'\n",
    "ndims = [5]\n",
    "bounds = [0, 1]\n",
    "N = [512] # number of nodes per hidden layer\n",
    "L = [2] # number of hidden layers\n",
    "activation = ['sin'] # activation function\n",
    "num_points = [100000, 1000000]\n",
    "use_sobol = [False, True] # whether to generate training points using Sobol sequence (True) or uniformly (False)\n",
    "batch_size = [4096]\n",
    "lr = [0.01]\n",
    "lr_scheduler = ['ReduceLROnPlateau-0.5-2']\n",
    "max_time = 300\n",
    "load_model = True # whether to load the model (True) or overwrite with a new one (False)\n",
    "\n",
    "for ndims, N, L, activation, num_points, use_sobol, batch_size, lr, lr_scheduler in itertools.product(ndims, N, L, activation, num_points, use_sobol, batch_size, lr, lr_scheduler):\n",
    "    name = f'{ndims},{bounds},{N},{L},{activation},{num_points},{use_sobol},{batch_size},{lr},{lr_scheduler}'\n",
    "    savedir = os.path.join(directory, name)\n",
    "\n",
    "    # Initialization\n",
    "    activation = {\n",
    "        'ELU': nn.ELU,\n",
    "        'sigmoid': nn.Sigmoid,\n",
    "        'tanh': nn.Tanh,\n",
    "        'sin': Sin,\n",
    "        'atan': Atan,\n",
    "    }[activation]()\n",
    "    torch.manual_seed(82196)\n",
    "    model = HelmholtzSolver(ndims, N, L, activation, bounds).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr)\n",
    "    if lr_scheduler == 'ExponentialLR-0.95':\n",
    "        lr_scheduler = ExponentialLR(optimizer, 0.95)\n",
    "    elif lr_scheduler == 'ReduceLROnPlateau-0.1-10':\n",
    "        lr_scheduler = ReduceLROnPlateau(optimizer, 'min', 0.1, 10)\n",
    "    elif lr_scheduler == 'ReduceLROnPlateau-0.5-2':\n",
    "        lr_scheduler = ReduceLROnPlateau(optimizer, 'min', 0.5, 2)\n",
    "\n",
    "    if load_model and os.path.exists(os.path.join(savedir, 'model.pt')):\n",
    "        checkpoint = torch.load(os.path.join(savedir, 'model.pt'))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        last_point = checkpoint['last_point']\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
    "        with open(os.path.join(savedir, 'loss(time).csv')) as datafile:\n",
    "            data = list(csv.reader(datafile, quoting=csv.QUOTE_NONNUMERIC))\n",
    "        del data[0]\n",
    "        epoch, elapsed_time, MSE_f_train, MSE_f, RMSE_u, lr = data[-1]\n",
    "        if last_point > 0:\n",
    "            del data[-1]\n",
    "        epoch_data, time_data, MSE_f_train_data, MSE_f_data, RMSE_u_data, lr_data = [list(x) for x in zip(*data)]\n",
    "    else:\n",
    "        epoch, elapsed_time, MSE_f_train, last_point = 0, 0, 0, 0\n",
    "        epoch_data, time_data, MSE_f_train_data, MSE_f_data, RMSE_u_data, lr_data = [], [], [], [], [], []\n",
    "\n",
    "    # Training\n",
    "    start_time = time() - elapsed_time\n",
    "    output_timestamp = save_timestamp = time()\n",
    "    print(f'\\n{name}\\nepoch | time  | train MSE_f | test MSE_f | test RMSE_u | lr')\n",
    "    while elapsed_time < max_time:\n",
    "        MSE_f_train, last_point = train(model, optimizer, num_points, use_sobol, batch_size, max_time - elapsed_time, MSE_f_train, last_point)\n",
    "        MSE_f, RMSE_u = test(model, num_points, 10000)\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        epoch += last_point/num_points\n",
    "        if last_point >= num_points:\n",
    "            if isinstance(lr_scheduler, ExponentialLR):\n",
    "                lr_scheduler.step()\n",
    "            elif isinstance(lr_scheduler, ReduceLROnPlateau):\n",
    "                lr_scheduler.step(MSE_f)\n",
    "            last_point = 0\n",
    "        elapsed_time = time() - start_time\n",
    "        epoch_data.append(epoch)\n",
    "        time_data.append(elapsed_time)\n",
    "        MSE_f_train_data.append(MSE_f_train)\n",
    "        MSE_f_data.append(MSE_f)\n",
    "        RMSE_u_data.append(RMSE_u)\n",
    "        lr_data.append(lr)\n",
    "        if time() - output_timestamp >= 1 and elapsed_time < max_time:\n",
    "            print(f'{epoch:5.0f} | {elapsed_time:5.0f} | {MSE_f_train:11.3e} | {MSE_f:10.3e} | {RMSE_u:11.4f} | {lr:.1e}')\n",
    "            output_timestamp = time()\n",
    "        if time() - save_timestamp >= 600 or elapsed_time >= max_time:\n",
    "            save_timestamp = time()\n",
    "            \n",
    "            # Saving\n",
    "            os.makedirs(savedir, exist_ok=True)\n",
    "            with open(os.path.join(savedir, 'loss(time).csv'), 'w+') as output:\n",
    "                output.write('\"epoch\",\"time\",\"train MSE_f\",\"test MSE_f\",\"test RMSE_u\",\"lr\"\\n')\n",
    "                output.writelines([f'{epoch_data[i]:.2f},{time_data[i]:.2f},{MSE_f_train_data[i]:.3e},{MSE_f_data[i]:.3e},{RMSE_u_data[i]:.3e},{lr_data[i]:.1e}\\n' for i in range(len(time_data))])\n",
    "            state_dict = {'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}\n",
    "            if lr_scheduler is not None:\n",
    "                state_dict['lr_scheduler_state_dict'] = lr_scheduler.state_dict()\n",
    "            state_dict['last_point'] = last_point\n",
    "            torch.save(state_dict, os.path.join(savedir, 'model.pt'))\n",
    "\n",
    "            # Plot loss(time) and RMSE(time)\n",
    "            fig, axs = subplots(1, 2, figsize=(20,7))\n",
    "            axs[0].semilogy(time_data, MSE_f_data)\n",
    "            axs[0].set_xlabel('time [sec]')\n",
    "            axs[0].set_ylabel('MSE_f')\n",
    "            axs[1].semilogy(time_data, RMSE_u_data)\n",
    "            axs[1].set_xlabel('time [sec]')\n",
    "            axs[1].set_ylabel('RMSE_u')\n",
    "            savefig(os.path.join(savedir, 'loss(time).pdf'), bbox_inches='tight')\n",
    "            close()\n",
    "\n",
    "            start_time += time() - save_timestamp # subtract time spent on saving from measured time\n",
    "            save_timestamp = time()\n",
    "\n",
    "    print(f'{epoch:5.2f} | {elapsed_time:5.0f} | {MSE_f_train:11.3e} | {MSE_f:10.3e} | {RMSE_u:11.4f} | {lr:.1e}')\n",
    "    with open('results.csv', 'a') as output:\n",
    "        output.write(f'{name},{epoch:.2f},{elapsed_time:.0f},{MSE_f_train:.3e},{MSE_f:.3e},{RMSE_u:.3e},{lr:.1e}\\n')\n",
    "\n",
    "    # Plot sin(x)\n",
    "    if ndims == 1:\n",
    "        x = torch.arange(0, 1, 0.001).to(device)\n",
    "    else:\n",
    "        x = torch.cartesian_prod(*[torch.arange(0, 1, 0.005)]*2, *[torch.tensor([1/4])]*(ndims-2)).to(device)\n",
    "    u = torch.prod(torch.sin(2*pi*x), -1, True) # analytical solution\n",
    "    x, u, pred = x.cpu().detach(), u.cpu().detach(), model(x).cpu().detach()\n",
    "    if ndims == 1:\n",
    "        figure(figsize=(10,3))\n",
    "        plot(x, u, '.', markersize=1)\n",
    "        plot(x, pred, '.', markersize=1)\n",
    "        xlabel('x')\n",
    "        ylabel('u')\n",
    "        savefig(os.path.join(savedir, 'sin(x).png'), bbox_inches='tight')\n",
    "        close()\n",
    "    else:\n",
    "        fig = figure(figsize=(20,10))\n",
    "        ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "        ax.scatter(x[:,0], x[:,1], pred, c=pred, cmap='coolwarm', antialiased=False)\n",
    "        title('u(x), NN solution')\n",
    "        ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "        ax.scatter(x[:,0], x[:,1], pred-u, c=pred-u, cmap='coolwarm', antialiased=False)\n",
    "        title('NN solution - analytical solution')\n",
    "        savefig(os.path.join(savedir, 'sin(x).png'), bbox_inches='tight')\n",
    "        close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zupPIO2rJH_U",
   "metadata": {
    "id": "zupPIO2rJH_U",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title HPO\n",
    "ndims = 5\n",
    "bounds = [0, 1]\n",
    "\n",
    "def trainable(params):\n",
    "    N = params['N'] if 'N' in params else 512\n",
    "    L = params['L'] if 'L' in params else 2\n",
    "    activation = params['activation'] if 'activation' in params else 'tanh'\n",
    "    num_points = params['num_points'] if 'num_points' in params else 1000000\n",
    "    use_sobol = params['use_sobol'] if 'use_sobol' in params else True\n",
    "    batch_size = params['batch_size'] if 'batch_size' in params else 1024\n",
    "    lr = params['lr'] if 'lr' in params else 0.01\n",
    "    lr_scheduler = params['lr_scheduler'] if 'lr_scheduler' in params else None\n",
    "    max_time = 300\n",
    "    name = f'{ndims},{bounds},{N},{L},{activation},{num_points},{use_sobol},{batch_size},{lr},{lr_scheduler}'\n",
    "\n",
    "    # Initialization\n",
    "    activation = {\n",
    "        'ELU': nn.ELU,\n",
    "        'sigmoid': nn.Sigmoid,\n",
    "        'tanh': nn.Tanh,\n",
    "        'sin': Sin,\n",
    "        'atan': Atan,\n",
    "    }[activation]()\n",
    "    torch.manual_seed(82196)\n",
    "    model = HelmholtzSolver(ndims, N, L, activation, bounds).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr)\n",
    "    if lr_scheduler == 'ExponentialLR-0.95':\n",
    "        lr_scheduler = ExponentialLR(optimizer, 0.95)\n",
    "    elif lr_scheduler == 'ReduceLROnPlateau-0.1-10':\n",
    "        lr_scheduler = ReduceLROnPlateau(optimizer, 'min', 0.1, 10)\n",
    "    elif lr_scheduler == 'ReduceLROnPlateau-0.5-2':\n",
    "        lr_scheduler = ReduceLROnPlateau(optimizer, 'min', 0.5, 2)\n",
    "\n",
    "    checkpoint = session.get_checkpoint()\n",
    "    if checkpoint:\n",
    "        with checkpoint.as_directory() as checkpoint_dir:\n",
    "            checkpoint = torch.load(os.path.join(checkpoint_dir, 'model.pt'))\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            last_point = checkpoint['last_point']\n",
    "            if lr_scheduler is not None:\n",
    "                lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])\n",
    "            with open(os.path.join(checkpoint_dir, 'loss(time).csv')) as datafile:\n",
    "                data = list(csv.reader(datafile, quoting=csv.QUOTE_NONNUMERIC))\n",
    "        del data[0]\n",
    "        epoch, elapsed_time, MSE_f_train, MSE_f, RMSE_u, lr = data[-1]\n",
    "        if last_point > 0:\n",
    "            del data[-1]\n",
    "        epoch_data, time_data, MSE_f_train_data, MSE_f_data, RMSE_u_data, lr_data = [list(x) for x in zip(*data)]\n",
    "    else:\n",
    "        epoch, elapsed_time, MSE_f_train, last_point = 0, 0, 0, 0\n",
    "        epoch_data, time_data, MSE_f_train_data, MSE_f_data, RMSE_u_data, lr_data = [], [], [], [], [], []\n",
    "\n",
    "    # Training\n",
    "    start_time = time() - elapsed_time\n",
    "    output_timestamp = save_timestamp = time()\n",
    "    print(f'\\n{name}\\nepoch | time  | train MSE_f | test MSE_f | test RMSE_u | lr')\n",
    "    while elapsed_time < max_time:\n",
    "        checkpoint = None\n",
    "        MSE_f_train, last_point = train(model, optimizer, num_points, use_sobol, batch_size, max_time - elapsed_time, MSE_f_train, last_point)\n",
    "        MSE_f, RMSE_u = test(model, num_points, 10000)\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        epoch += last_point/num_points\n",
    "        if last_point >= num_points:\n",
    "            if isinstance(lr_scheduler, ExponentialLR):\n",
    "                lr_scheduler.step()\n",
    "            elif isinstance(lr_scheduler, ReduceLROnPlateau):\n",
    "                lr_scheduler.step(MSE_f)\n",
    "            last_point = 0\n",
    "        elapsed_time = time() - start_time\n",
    "        epoch_data.append(epoch)\n",
    "        time_data.append(elapsed_time)\n",
    "        MSE_f_train_data.append(MSE_f_train)\n",
    "        MSE_f_data.append(MSE_f)\n",
    "        RMSE_u_data.append(RMSE_u)\n",
    "        lr_data.append(lr)\n",
    "        if time() - output_timestamp >= 1 and elapsed_time < max_time:\n",
    "            print(f'{epoch:5.0f} | {elapsed_time:5.0f} | {MSE_f_train:11.3e} | {MSE_f:10.3e} | {RMSE_u:11.4f} | {lr:.1e}')\n",
    "            output_timestamp = time()\n",
    "        if time() - save_timestamp >= 600 or elapsed_time >= max_time:\n",
    "            save_timestamp = time()\n",
    "            \n",
    "            # Saving\n",
    "            os.makedirs('data', exist_ok=True)\n",
    "            with open(os.path.join('data', 'loss(time).csv'), 'w+') as output:\n",
    "                output.write('\"epoch\",\"time\",\"train MSE_f\",\"test MSE_f\",\"test RMSE_u\",\"lr\"\\n')\n",
    "                output.writelines([f'{epoch_data[i]:.2f},{time_data[i]:.2f},{MSE_f_train_data[i]:.3e},{MSE_f_data[i]:.3e},{RMSE_u_data[i]:.3e},{lr_data[i]:.1e}\\n' for i in range(len(time_data))])\n",
    "            state_dict = {'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}\n",
    "            if lr_scheduler is not None:\n",
    "                state_dict['lr_scheduler_state_dict'] = lr_scheduler.state_dict()\n",
    "            state_dict['last_point'] = last_point\n",
    "            torch.save(state_dict, os.path.join('data', 'model.pt'))\n",
    "            checkpoint = Checkpoint.from_directory('data')\n",
    "\n",
    "            # Plot loss(time) and RMSE(time)\n",
    "            fig, axs = subplots(1, 2, figsize=(20,7))\n",
    "            axs[0].semilogy(time_data, MSE_f_data)\n",
    "            axs[0].set_xlabel('time [sec]')\n",
    "            axs[0].set_ylabel('MSE_f')\n",
    "            axs[1].semilogy(time_data, RMSE_u_data)\n",
    "            axs[1].set_xlabel('time [sec]')\n",
    "            axs[1].set_ylabel('RMSE_u')\n",
    "            savefig(os.path.join('data', 'loss(time).pdf'), bbox_inches='tight')\n",
    "            close()\n",
    "\n",
    "            start_time += time() - save_timestamp # subtract time spent on saving from measured time\n",
    "            save_timestamp = time()\n",
    "\n",
    "        session.report({'elapsed_time': elapsed_time, 'MSE_f_train': MSE_f_train, 'MSE_f': MSE_f, 'RMSE_u': RMSE_u, 'lr': lr}, checkpoint=checkpoint)\n",
    "\n",
    "name = f'{ndims=},RandomSearch'\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(\n",
    "        trainable,\n",
    "        resources={'cpu': 1, 'gpu': 1}\n",
    "    ),\n",
    "    param_space={\n",
    "        'N': tune.choice([128, 256, 512, 1024, 2048]),\n",
    "        'L': tune.choice([1, 2, 3, 4]),\n",
    "        'activation': tune.choice(['ELU', 'sigmoid', 'tanh', 'sin', 'atan']),\n",
    "        'num_points': tune.choice([1000, 10000, 100000, 1000000]),\n",
    "        'use_sobol': tune.choice([False, True]),\n",
    "        'batch_size': tune.choice([64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]),\n",
    "        'lr': tune.choice([1e-4, 1e-3, 1e-2, 1e-1, 1]),\n",
    "        'lr_scheduler': tune.choice([None, 'ExponentialLR-0.95', 'ReduceLROnPlateau-0.1-10', 'ReduceLROnPlateau-0.5-2']),\n",
    "    },\n",
    "    tune_config=tune.TuneConfig(\n",
    "        mode=\"min\",\n",
    "        metric=\"MSE_f\",\n",
    "        # search_alg=HyperOptSearch(random_state_seed=2023),\n",
    "        # scheduler=ASHAScheduler(time_attr='elapsed_time', max_t=300),\n",
    "        # search_alg=TuneBOHB(seed=2023)\n",
    "        # scheduler=HyperBandForBOHB(time_attr='elapsed_time', max_t=300)\n",
    "        num_samples=-1,\n",
    "        time_budget_s=8*3600,\n",
    "    ),\n",
    "    run_config = RunConfig(\n",
    "        name=name,\n",
    "        local_dir='ray_results',\n",
    "        progress_reporter=CLIReporter(max_report_frequency=60),\n",
    "    )\n",
    ")\n",
    "# tuner = tune.Tuner.restore(os.path.join('ray_results', name), tune.with_resources(trainable, resources={'cpu': 1, 'gpu': 1}))\n",
    "results = tuner.fit()\n",
    "df = results.get_dataframe()\n",
    "df.to_csv(f'HPO_dataframes/{name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2JyFwihO8ZcN",
   "metadata": {
    "id": "2JyFwihO8ZcN"
   },
   "outputs": [],
   "source": [
    "#@title Load tuning results\n",
    "name = f'{ndims=},RandomSearch'\n",
    "tuner = tune.Tuner.restore(os.path.join('ray_results', name), tune.with_resources(trainable, resources={'cpu': 1, 'gpu': 1}))\n",
    "results = tuner.get_results()\n",
    "df = results.get_dataframe()\n",
    "df.to_csv(f'HPO_dataframes/{name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cGhqIGUfouNF",
   "metadata": {
    "id": "cGhqIGUfouNF",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#@title Compare loss(time) plots\n",
    "directory = 'models/ndims=8'\n",
    "ndims = [8]\n",
    "bounds = [0, 1]\n",
    "N = [1024] # number of nodes per hidden layer\n",
    "L = [1] # number of hidden layers\n",
    "activation = ['sin'] # activation function\n",
    "num_points = [10000000]\n",
    "use_sobol = [True] # whether to generate training points using Sobol sequence (True) or uniformly (False)\n",
    "batch_size = [4096]\n",
    "lr = [0.1]\n",
    "lr_scheduler = ['ReduceLROnPlateau-0.5-2']\n",
    "\n",
    "figure()\n",
    "for ndims, N, L, activation, num_points, use_sobol, batch_size, lr, lr_scheduler in itertools.product(ndims, N, L, activation, num_points, use_sobol, batch_size, lr, lr_scheduler):\n",
    "    name = f'{ndims},{bounds},{N},{L},{activation},{num_points},{use_sobol},{batch_size},{lr},{lr_scheduler}'\n",
    "    if os.path.isdir(os.path.join(directory, name)):\n",
    "        with open(os.path.join(directory, name, 'loss(time).csv')) as datafile:\n",
    "            data = list(csv.reader(datafile, quoting=csv.QUOTE_NONNUMERIC))\n",
    "        del data[0]\n",
    "        poch_data, time_data, MSE_f_train_data, MSE_f_data, RMSE_u_data, lr_data = [list(x) for x in zip(*data)]\n",
    "        semilogy(time_data, MSE_f_data, label=f'num_points={num_points}, use_sobol={use_sobol}')\n",
    "title(f'ndims={ndims}, N={N}, L={L}, σ={activation}, batch_size={batch_size}, lr={lr}, lr_scheduler={lr_scheduler}', fontsize=10)\n",
    "xlabel('time [sec]')\n",
    "ylabel('MSE_f')\n",
    "legend()\n",
    "savefig(os.path.join(directory, 'loss(time).pdf'), bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
